{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predicted: morning\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Predict next word using GPT-2 model\n",
    "def predict_next_word(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=len(input_ids[0])+1, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_token_ids = output[0][len(input_ids[0])]\n",
    "    predicted_word = tokenizer.decode(predicted_token_ids)\n",
    "    return predicted_word.strip()\n",
    "\n",
    "# Example usage\n",
    "text = \"Sun rises in the\"\n",
    "predicted_word = predict_next_word(text)\n",
    "print(\"Next word predicted:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predicted (greedy): morning\n",
      "Next word predicted (sampling): ,\n",
      "Alternative predictions (sampling): ['and']\n",
      "Next word predicted (top-k sampling): rises in the night.\n",
      "Alternative predictions (top-k sampling): ['rises in the distance.', 'rises in the afternoon, and']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to predict next word using greedy decoding\n",
    "def predict_next_word(text):\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=len(input_ids[0])+1, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "  predicted_token_ids = output[0][len(input_ids[0])]\n",
    "  predicted_word = tokenizer.decode(predicted_token_ids).strip()\n",
    "  return predicted_word\n",
    "\n",
    "# Function to predict next word and alternatives using sampling\n",
    "def predict_next_words_sampling(text, num_alternatives=2):\n",
    "  \"\"\"\n",
    "  Predicts the next word and a specified number of alternatives using sampling.\n",
    "\n",
    "  Args:\n",
    "      text: The input text for which to predict the next word.\n",
    "      num_alternatives: The number of alternative predictions to return (default: 2).\n",
    "\n",
    "  Returns:\n",
    "      A list containing the predicted next word and the specified number of alternatives.\n",
    "  \"\"\"\n",
    "  predictions = []\n",
    "  for _ in range(num_alternatives + 1):\n",
    "    predicted_word = predict_next_word(text)\n",
    "    predictions.append(predicted_word)\n",
    "    text += \" \" + predicted_word  # Update text for next prediction\n",
    "  return predictions[1:]  # Exclude the first prediction (which is the last word of the input)\n",
    "\n",
    "# Function to predict next word and alternatives using top-k sampling\n",
    "def predict_next_words_topk(text, num_alternatives=2, top_k=50):\n",
    "  \"\"\"\n",
    "  Predicts the next word and a specified number of alternatives using top-k sampling.\n",
    "\n",
    "  Args:\n",
    "      text: The input text for which to predict the next word.\n",
    "      num_alternatives: The number of alternative predictions to return (default: 2).\n",
    "      top_k: The number of highest probability tokens to consider during sampling (default: 50).\n",
    "\n",
    "  Returns:\n",
    "      A list containing the predicted next word and the specified number of alternatives.\n",
    "  \"\"\"\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=len(input_ids[0])+num_alternatives+1, \n",
    "                             do_sample=True, top_k=top_k, no_repeat_ngram_size=2, \n",
    "                             pad_token_id=tokenizer.eos_token_id, num_return_sequences=num_alternatives+1)\n",
    "  predicted_tokens = output[:, 1:]\n",
    "  predicted_words = [tokenizer.decode(token_ids).strip() for token_ids in predicted_tokens]\n",
    "  return [predicted_words[0]] + predicted_words[1:]\n",
    "\n",
    "# Example usage\n",
    "text = \"Sun rises in the\"\n",
    "\n",
    "# Using greedy decoding (might not provide alternatives)\n",
    "predicted_word = predict_next_word(text)\n",
    "print(\"Next word predicted (greedy):\", predicted_word)\n",
    "\n",
    "# Using sampling (provides alternatives with some randomness)\n",
    "predicted_words = predict_next_words_sampling(text, num_alternatives=2)\n",
    "print(\"Next word predicted (sampling):\", predicted_words[0])\n",
    "print(\"Alternative predictions (sampling):\", predicted_words[1:])\n",
    "\n",
    "# Using top-k sampling (provides alternatives with more control)\n",
    "predicted_words = predict_next_words_topk(text, num_alternatives=2, top_k=30)\n",
    "print(\"Next word predicted (top-k sampling):\", predicted_words[0])\n",
    "print(\"Alternative predictions (top-k sampling):\", predicted_words[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words predicted:\n",
      "1: m\n",
      "2: o\n",
      "3: r\n",
      "4: n\n",
      "5: i\n",
      "6: n\n",
      "7: g\n",
      "8: ,\n",
      "9: and\n",
      "10: rises in the face of war\n",
      "11: rises in the late afternoon this\n",
      "12: rises in the sky with its\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to predict next word using greedy decoding\n",
    "def predict_next_word(text):\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=len(input_ids[0])+1, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "  predicted_token_ids = output[0][len(input_ids[0])]\n",
    "  predicted_word = tokenizer.decode(predicted_token_ids).strip()\n",
    "  return predicted_word\n",
    "\n",
    "# Function to predict next word and alternatives using sampling\n",
    "def predict_next_words_sampling(text, num_alternatives=2):\n",
    "  \"\"\"\n",
    "  Predicts the next word and a specified number of alternatives using sampling.\n",
    "\n",
    "  Args:\n",
    "      text: The input text for which to predict the next word.\n",
    "      num_alternatives: The number of alternative predictions to return (default: 2).\n",
    "\n",
    "  Returns:\n",
    "      A list containing the predicted next word and the specified number of alternatives.\n",
    "  \"\"\"\n",
    "  predictions = []\n",
    "  for _ in range(num_alternatives + 1):\n",
    "    predicted_word = predict_next_word(text)\n",
    "    predictions.append(predicted_word)\n",
    "    text += \" \" + predicted_word  # Update text for next prediction\n",
    "  return predictions[1:]  # Exclude the first prediction (which is the last word of the input)\n",
    "\n",
    "# Function to predict next word and alternatives using top-k sampling\n",
    "def predict_next_words_topk(text, num_alternatives=2, top_k=50):\n",
    "  \"\"\"\n",
    "  Predicts the next word and a specified number of alternatives using top-k sampling.\n",
    "\n",
    "  Args:\n",
    "      text: The input text for which to predict the next word.\n",
    "      num_alternatives: The number of alternative predictions to return (default: 2).\n",
    "      top_k: The number of highest probability tokens to consider during sampling (default: 50).\n",
    "\n",
    "  Returns:\n",
    "      A list containing the predicted next word and the specified number of alternatives.\n",
    "  \"\"\"\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=len(input_ids[0])+num_alternatives+1, \n",
    "                             do_sample=True, top_k=top_k, no_repeat_ngram_size=2, \n",
    "                             pad_token_id=tokenizer.eos_token_id, num_return_sequences=num_alternatives+1)\n",
    "  predicted_tokens = output[:, 1:]\n",
    "  predicted_words = [tokenizer.decode(token_ids).strip() for token_ids in predicted_tokens]\n",
    "  return [predicted_words[0]] + predicted_words[1:]\n",
    "\n",
    "# Example usage\n",
    "text = \"Sun rises in the\"\n",
    "\n",
    "# Combine predictions from all methods\n",
    "all_predictions = []\n",
    "methods = [predict_next_word, predict_next_words_sampling, predict_next_words_topk]\n",
    "for i, method in enumerate(methods):\n",
    "  predictions = method(text)\n",
    "  all_predictions.extend(predictions)\n",
    "\n",
    "# Print predictions with numbering\n",
    "print(\"Next words predicted:\")\n",
    "for i, prediction in enumerate(all_predictions, start=1):\n",
    "  print(f\"{i}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rio/anaconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative next words predicted: ['fence', 'bridge', 'edge', 'road', 'side']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "def predict_next_words(text, num_predictions=3):\n",
    "    # Ensure num_predictions does not exceed num_beams\n",
    "    num_beams = max(num_predictions, 3)\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0])+1,\n",
    "            num_return_sequences=num_predictions,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    predicted_words = []\n",
    "    for output in outputs:\n",
    "        predicted_token_id = output[-1]  # Get the last token from each output\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "        predicted_words.append(predicted_word.strip())\n",
    "    \n",
    "    return predicted_words\n",
    "\n",
    "# Example usage\n",
    "text = \"The car drove over the\"\n",
    "predicted_words = predict_next_words(text, num_predictions=5)\n",
    "print(\"Alternative next words predicted:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative next words predicted: ['fence', 'bridge', 'edge', 'road', 'side']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def predict_next_words(text, num_predictions=3):\n",
    "    # Ensure the number of beams is at least equal to the number of predictions desired\n",
    "    num_beams = max(num_predictions, 3)\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        # Generate sequences\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + 1,\n",
    "            num_return_sequences=num_predictions,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode each output sequence to get the predicted next word\n",
    "    predicted_words = []\n",
    "    for output in outputs:\n",
    "        predicted_token_id = output[-1]  # Get the last token from each output\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "        predicted_words.append(predicted_word.strip())\n",
    "    \n",
    "    return predicted_words\n",
    "\n",
    "# Example usage\n",
    "text = \"The car drove over the\"\n",
    "predicted_words = predict_next_words(text, num_predictions=5)\n",
    "print(\"Alternative next words predicted:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence scores not available in the output.\n",
      "Predicted word: 'fence' with score: None\n",
      "Predicted word: 'bridge' with score: None\n",
      "Predicted word: 'edge' with score: None\n",
      "Predicted word: 'road' with score: None\n",
      "Predicted word: 'side' with score: None\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Ensure you have an appropriate version of the transformers library\n",
    "# pip install transformers\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def predict_next_words_with_beam_scores(text, num_predictions=3):\n",
    "    num_beams = max(num_predictions, 3)\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + 1,\n",
    "            num_return_sequences=num_predictions,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    # Check if 'sequences_scores' is in the outputs\n",
    "    if 'sequences_scores' in outputs:\n",
    "        sequence_scores = outputs['sequences_scores'].tolist()\n",
    "    else:\n",
    "        # Fallback or alternative handling if 'sequences_scores' is not available\n",
    "        print(\"Sequence scores not available in the output.\")\n",
    "        sequence_scores = [None] * num_predictions  # Placeholder scores\n",
    "\n",
    "    predicted_words_with_scores = []\n",
    "    for idx, sequence in enumerate(outputs['sequences']):\n",
    "        predicted_token_id = sequence[-1].item()  # Get the last token from each sequence\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "        score = sequence_scores[idx]  # Get the score for the sequence (if available)\n",
    "        predicted_words_with_scores.append((predicted_word.strip(), score))\n",
    "    \n",
    "    return predicted_words_with_scores\n",
    "\n",
    "# Example usage\n",
    "text = \"The car drove over the\"\n",
    "predicted_words_with_scores = predict_next_words_with_beam_scores(text, num_predictions=5)\n",
    "for word, score in predicted_words_with_scores:\n",
    "    print(f\"Predicted word: '{word}' with score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence scores not available in the output.\n",
      "Predicted word: 'fence' with score: nan%\n",
      "Predicted word: 'bridge' with score: nan%\n",
      "Predicted word: 'edge' with score: nan%\n",
      "Predicted word: 'road' with score: nan%\n",
      "Predicted word: 'side' with score: nan%\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def predict_next_words_with_percentage_scores(text, num_predictions=3):\n",
    "    num_beams = max(num_predictions, 3)\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + 1,\n",
    "            num_return_sequences=num_predictions,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    # Extract sequence scores if available\n",
    "    if 'sequences_scores' in outputs:\n",
    "        sequence_scores = outputs['sequences_scores']\n",
    "        # Convert scores to probabilities (percentage)\n",
    "        probabilities = F.softmax(sequence_scores, dim=0) * 100\n",
    "    else:\n",
    "        print(\"Sequence scores not available in the output.\")\n",
    "        probabilities = torch.full((num_predictions,), float('nan'))  # Placeholder scores\n",
    "\n",
    "    predicted_words_with_percentage_scores = []\n",
    "    for idx, sequence in enumerate(outputs['sequences']):\n",
    "        predicted_token_id = sequence[-1].item()  # Get the last token from each sequence\n",
    "        predicted_word = tokenizer.decode(predicted_token_id)\n",
    "        percentage_score = probabilities[idx].item()  # Get the percentage score for the sequence\n",
    "        predicted_words_with_percentage_scores.append((predicted_word.strip(), f\"{percentage_score:.2f}%\"))\n",
    "    \n",
    "    return predicted_words_with_percentage_scores\n",
    "\n",
    "# Example usage\n",
    "text = \"The car drove over the\"\n",
    "predicted_words_with_percentage_scores = predict_next_words_with_percentage_scores(text, num_predictions=5)\n",
    "for word, score in predicted_words_with_percentage_scores:\n",
    "    print(f\"Predicted word: '{word}' with score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words predicted:\n",
      "Option 1: evening on the island of Gentua - but no captain would survive it.\n",
      "Option 2: apron on this plastic roof.\n",
      "\n",
      "Frenzy covering the windows.\n",
      "Option 3: temperature, says Andrea Horner, Research Director at The Climate Institute, in New\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Predict next words using GPT-2 model with nucleus sampling\n",
    "def predict_next_words(text, num_alternatives=3, temperature=1.0, top_p=0.9, max_length=20):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    predicted_words = []\n",
    "    for _ in range(num_alternatives):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=0, top_p=top_p, temperature=temperature, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "        predicted_token_ids = output[0][len(input_ids[0]):]\n",
    "        predicted_word = tokenizer.decode(predicted_token_ids).strip()\n",
    "        predicted_words.append(predicted_word)\n",
    "    return predicted_words\n",
    "\n",
    "# Example usage\n",
    "text = \"Sun rises in the\"\n",
    "predicted_words = predict_next_words(text)\n",
    "print(\"Next words predicted:\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(f\"Option {i+1}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words predicted:\n",
      "1: morning\n",
      "2: ,\n",
      "3: and\n",
      "4: the\n",
      "5: sun\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Predict next words using GPT-2 model\n",
    "def predict_next_words(text, num_words=5):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=len(input_ids[0])+num_words, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_token_ids = output[0][len(input_ids[0]):]\n",
    "    predicted_words = []\n",
    "    for token_ids in predicted_token_ids:\n",
    "        predicted_word = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        predicted_words.append(predicted_word.strip())\n",
    "    return predicted_words\n",
    "\n",
    "# Example usage\n",
    "text = \"Sun rises in the\"\n",
    "predicted_words = predict_next_words(text)\n",
    "print(\"Next words predicted:\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(f\"{i+1}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words predicted:\n",
      "Option 1: the first 200 meters and near the 100,\" he said.\n",
      "\n",
      "\"But maybe\n",
      "Option 2: the western Mediterranean Sea but you'll find that the price of bananas is around 80 cents\n",
      "Option 3: the west and leaves a rift in the sun's brightness.\"\n",
      "\n",
      "Roger Waters,\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Predict next words using GPT-2 model with nucleus sampling\n",
    "def predict_next_words(text, num_alternatives=3, temperature=1.0, top_p=0.9, max_length=20):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    predicted_words = []\n",
    "    for _ in range(num_alternatives):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=0, top_p=top_p, temperature=temperature, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "        predicted_token_ids = output[0][len(input_ids[0])-1:]  # Extract only the last token\n",
    "        predicted_word = tokenizer.decode(predicted_token_ids).strip()\n",
    "        predicted_words.append(predicted_word)\n",
    "    return predicted_words\n",
    "\n",
    "# Example usage\n",
    "text = \"Sun rises in the\"\n",
    "predicted_words = predict_next_words(text)\n",
    "print(\"Next words predicted:\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(f\"Option {i+1}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rio/anaconda3/envs/py310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chardet transformers "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
